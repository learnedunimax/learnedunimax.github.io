<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" /> -->
<!--   <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>LearnedUniMax</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Learning Uniformly Distributed Embedding Clusters of Stylistic Skills for Physically Simulated Characters</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="#">Nian Liu</a><sup>1,2</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="#">Libin Liu</a><sup>3</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="#">Zilong Zhang</a><sup>1</sup>,</span>
                    <span class="author-block">  
                    <a href="THIRD AUTHOR PERSONAL LINK" target="#">Zi Wang</a><sup>1</sup>,</span>
                  <span class="author-block">  
                    <a href="THIRD AUTHOR PERSONAL LINK" target="#">Hongzhao Xie</a><sup>2</sup>,</span>
                  <span class="author-block">  
                    <a href="THIRD AUTHOR PERSONAL LINK" target="#">Tengyu Liu</a><sup>2</sup>,</span>
                  <span class="author-block">  
                    <a href="THIRD AUTHOR PERSONAL LINK" target="#">Xinyi Tong</a><sup>2,4</sup>,</span>
                  <span class="author-block">  
                    <a href="THIRD AUTHOR PERSONAL LINK" target="#">Yaodong Yang</a><sup>3</sup>,</span>
                  <span class="author-block">  
                    <a href="THIRD AUTHOR PERSONAL LINK" target="#">Zhaofeng He</a><sup>1</sup>,</span>
                  <span class="author-block">  
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Beijing University of Posts and Telecommunications,</span>
                    <span class="author-block"><sup>2</sup>National Key Laboratory of General Artificial Intelligence, BIGAI,</span>
                    <span class="author-block"><sup>3</sup>Peking University,</span>
                    <span class="author-block"><sup>4</sup>Central Conservatory of Music</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
<!--                       <span class="link-block">
                        <a href="https://arxiv.org/pdf/2411.06459.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
<!--                     <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2411.06459.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="#"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming soon)</span>
                  </a>
                </span>


                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/final_demo.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Our controller not only generates high-quality, diverse motions covering the entire dataset but also achieves superior controllability, serving as a cornerstone for diverse applications.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Learning natural and diverse behaviors from human motion datasets remains a significant challenge in physics-based character control. Existing conditional adversarial models often suffer from tight and biased embedding distributions where embeddings from the same motion are closely grouped in a small area and shorter motions occupy even less space. Our empirical observations indicate this limits the representational capacity and diversity under each skill. An ideal latent space should be maximally packed by all motion's embedding clusters. Although methods that employ separate embedding space for each motion mitigate this limitation to some extent, introducing a hybrid discrete-continuous embedding space imposes a huge exploration burden on the high-level policy. To address the above limitations, we propose a versatile skill-conditioned controller that learns diverse skills with expressive variations. Our approach leverages the Neural Collapse phenomenon, a natural outcome of the classification-based encoder, to uniformly distributed cluster centers. We additionally propose a novel Embedding Expansion technique to form stylistic embedding clusters for diverse skills that are uniformly distributed on a hypersphere, maximizing the representational area occupied by each skill and minimizing unmapped regions. This maximally packed and uniformly distributed embedding space ensures that embeddings within the same cluster generate behaviors conforming to the characteristics of the corresponding motion clips, yet exhibiting noticeable variations within each cluster. Compared to existing methods, experimental results demonstrate that our controller not only generates high-quality, diverse motions covering the entire dataset but also achieves superior controllability, motion coverage, and diversity under each skill. Both qualitative and quantitative results confirm these traits, enabling our controller to be applied to a wide range of downstream tasks and serving as a cornerstone for diverse applications.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <div class="level-set has-text-justified">
            <img src="static/images/pipeline.png" alt="Recovering the Pre-Fine-Tuning Weight of an Aligned Mistral 7B" class="blend-img-background center-image"/>
            <p>
  Our method uses a unit hypersphere as the embedding space to feature uniformly distributed embedding clusters for each skill. We first employ a classification-based encoder to distribute motion features uniformly on a high-dimensional sphere, then apply conditional imitation learning with the Embedding Expansion technique to form a stylistic skill embedding cluster for each skill, achieving a maximally packed and uniformly distributed space
          </p>
        </div>
        </div>
        
      </div>
    </div>
  </div>
 </div>
</section>


<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Skill Showing</h2>
      <div class="item item-video4">
        <div class="item has-text-centered">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/one_character_showing.mp4"
            type="video/mp4">
          </video>

        </div>
      </div>
      <div class="item item-video4">
        <div class="item has-text-centered">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/sword_character_showing.mp4"
            type="video/mp4">
          </video>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Diversity under Skill</h2>
      <div class="item item-video4">
        <div class="item has-text-centered">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/diversity_29_showing.mp4"
            type="video/mp4">
          </video>

        </div>
      </div>
      <div class="item item-video4">
        <div class="item has-text-centered">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/diversity_116_showing.mp4"
            type="video/mp4">
          </video>

        </div>
      </div>
      <div class="item item-video3">
        <div class="item has-text-centered">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/diversity_dodge_right_showing.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-8">
          <video poster="" id="8" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/complete_8.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-29">
          <video poster="" id="29" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/complete_29.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-13">
          <video poster="" id="13" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/complete_13.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-9">
          <video poster="" id="9" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/complete_9.mp4"
                    type="video/mp4">
          </video>
        </div>
        </div>
          Our controller not only learns a diverse repertoire of motor skills from reference motions to synthesize  natural strategies in complex downstream tasks but also guarantee the execution of full movements, such as kinds of combos in video games, to clearly convey motion intentions.
          </p>
      </div>
    </div>
  </div>
</section>



<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div id="slider-new-carousel" class="slider" tabindex="0">
          <div class="slider-container" style="opacity: 1; width: 8512px; transition: none 0s ease 0s; transform: translate3d(-1792px, 0px, 0px);">
            <div class="slider-item is-slide-next" data-slider-index="0" style="width: 448px;">
              <div class="item item-video1">
                <video poster="" id="video1" autoplay controls muted loop playsinline height="100%">
                  <source src="static/videos/carousel1.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <div class="slider-item" data-slider-index="1" style="width: 448px;">
              <div class="item item-video2">
                <video poster="" id="video2" autoplay controls muted loop playsinline height="100%">
                  <source src="static/videos/carousel2.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <div class="slider-item" data-slider-index="2" style="width: 448px;">
              <div class="item item-video3">
                <video poster="" id="video3" autoplay controls muted loop playsinline height="100%">
                  <source src="static/videos/carousel3.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
          <div class="slider-navigation-previous">
            <svg viewBox="0 0 50 80" xml:space="preserve">
              <polyline fill="currentColor" stroke-width=".5em" stroke-linecap="round" stroke-linejoin="round" points="45.63,75.8 0.375,38.087 45.63,0.375 "></polyline>
            </svg>
          </div>
          <div class="slider-navigation-next">
            <svg viewBox="0 0 50 80" xml:space="preserve">
              <polyline fill="currentColor" stroke-width=".5em" stroke-linecap="round" stroke-linejoin="round" points="0.375,0.375 45.63,38.087 0.375,75.8 "></polyline>
            </svg>
          </div>
          <div class="slider-pagination">
            <div class="slider-page is-active" data-index="0"></div>
            <div class="slider-page" data-index="1"></div>
            <div class="slider-page" data-index="2"></div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>





  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
